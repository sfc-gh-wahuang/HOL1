{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark.functions import col, lit, current_timestamp\n",
    "from snowflake.snowpark.dataframe import DataFrame\n",
    "\n",
    "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd4dd8-a58b-4043-b041-02040efa110a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell56"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.account = session.get_current_account()\n",
    "        self.role = session.get_current_role()\n",
    "        self.warehouse = session.get_current_warehouse()\n",
    "        self.database = session.get_current_database()\n",
    "        self.schema = session.get_current_schema()\n",
    "\n",
    "config = Config()\n",
    "config.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "### List of table names and column names for easier reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "application_record_table_name = \"APPLICATION_RECORD\"\n",
    "credit_record_table_name = \"CREDIT_RECORD\"\n",
    "\n",
    "model_name = \"CREDIT_RISK_XGBoost\"\n",
    "\n",
    "application_record_fv_name = \"application_record_features\"\n",
    "\n",
    "id_col = \"ID\"\n",
    "target_col=\"TARGET\"\n",
    "output_col=\"PREDICT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "### 0. Set up feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"  |\"VERSION\"  |\"DATABASE_NAME\"  |\"SCHEMA_NAME\"  |\"CREATED_ON\"  |\"OWNER\"  |\"DESC\"  |\"ENTITIES\"  |\"REFRESH_FREQ\"  |\"REFRESH_MODE\"  |\"SCHEDULING_STATE\"  |\"WAREHOUSE\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|        |           |                 |               |              |         |        |            |                |                |                    |             |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    default_warehouse=config.warehouse,\n",
    "    database=config.database,\n",
    "    name=\"application_credit_feature_store\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "feature_views_df = fs.list_feature_views()\n",
    "feature_views_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "### 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ID\"     |\"CODE_GENDER\"  |\"FLAG_OWN_CAR\"  |\"FLAG_OWN_REALTY\"  |\"AMT_INCOME_TOTAL\"  |\"NAME_INCOME_TYPE\"  |\"NAME_EDUCATION_TYPE\"  |\"NAME_FAMILY_STATUS\"  |\"NAME_HOUSING_TYPE\"  |\"FLAG_MOBIL\"  |\"FLAG_WORK_PHONE\"  |\"FLAG_PHONE\"  |\"FLAG_EMAIL\"  |\"CNT_CHILDREN_IND\"  |\"CNT_FAMILY_IND\"  |\"AGE\"  |\"WORKYEAR\"  |\"OCCUPATION_TYPE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|5008804  |M              |Y               |Y                  |427500.00           |Working             |Higher education       |Civil marriage        |Rented apartment     |1             |1                  |0             |0             |0                   |2                 |33     |13          |OTHER              |\n",
      "|5008805  |M              |Y               |Y                  |427500.00           |Working             |Higher education       |Civil marriage        |Rented apartment     |1             |1                  |0             |0             |0                   |2                 |33     |13          |OTHER              |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "application_record = session.table(f'{config.database}.{config.schema}.{application_record_table_name}')\n",
    "\n",
    "def feature_engineering(application_record):\n",
    "    \"\"\"\n",
    "    Applies feature engineering to the application record.\n",
    "    \"\"\"\n",
    "    return application_record.with_column(\n",
    "        'CNT_CHILDREN_IND', F.iff(F.col('CNT_CHILDREN') >= 2, \"2+\", F.to_varchar(F.col('CNT_CHILDREN')))\n",
    "    ).drop('CNT_CHILDREN').with_column(\n",
    "        'CNT_FAMILY_IND', F.iff(F.col('CNT_FAM_MEMBERS') >= 3, \"3+\", F.to_varchar(F.col('CNT_FAM_MEMBERS')))\n",
    "    ).drop('CNT_FAM_MEMBERS').with_column(\n",
    "        'AGE', F.abs(F.floor(F.col('DAYS_BIRTH') / 365))\n",
    "    ).drop('DAYS_BIRTH').with_column(\n",
    "        'WORKYEAR', F.abs(F.floor(F.col('DAYS_EMPLOYED') / 365))\n",
    "    ).filter(F.col('WORKYEAR') < 50).drop('DAYS_EMPLOYED').with_column(\n",
    "        'OCCUPATION_TYPE',\n",
    "        F.iff(F.col('OCCUPATION_TYPE').in_(['Cleaning staff', 'Cooking staff', 'Drivers', 'Laborers', 'Low-skill Laborers', 'Security staff', 'Waiters/barmen staff']), 'LABOURWORK',\n",
    "              F.iff(F.col('OCCUPATION_TYPE').in_(['Accountants', 'Core staff', 'HR staff', 'Medicine staff', 'Private service staff', 'Realty agents', 'Sales staff', 'Secretaries']), 'OFFICEWORK', \n",
    "                    F.iff(F.col('OCCUPATION_TYPE').in_(['Managers', 'High skill tech staff', 'IT staff']), 'HIGHTTECHWORK', 'OTHER')))\n",
    "    )\n",
    "\n",
    "application_record = feature_engineering(application_record)\n",
    "application_record.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "|\"ID\"     |\"TARGET\"  |\n",
      "----------------------\n",
      "|5001711  |0         |\n",
      "|5001712  |0         |\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit_record = session.table(f'{config.database}.{config.schema}.{credit_record_table_name}')\n",
    "\n",
    "def process_labels(credit_record: DataFrame):\n",
    "    credit_record = credit_record.group_by('ID')\\\n",
    "        .agg(F.sum(F.iff(F.col('STATUS').in_(['2', '3', '4', '5']), 1, 0)).as_(\"CNT_LATE\"))\\\n",
    "        .with_column(target_col, F.when(F.col('CNT_LATE') > 0, 1).otherwise(0))\\\n",
    "        .drop(\"CNT_LATE\")\n",
    "\n",
    "    return credit_record\n",
    "\n",
    "credit_record = process_labels(credit_record)\n",
    "credit_record.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ID\"     |\"CODE_GENDER\"  |\"FLAG_OWN_CAR\"  |\"FLAG_OWN_REALTY\"  |\"AMT_INCOME_TOTAL\"  |\"NAME_INCOME_TYPE\"  |\"NAME_EDUCATION_TYPE\"  |\"NAME_FAMILY_STATUS\"  |\"NAME_HOUSING_TYPE\"  |\"FLAG_MOBIL\"  |\"FLAG_WORK_PHONE\"  |\"FLAG_PHONE\"  |\"FLAG_EMAIL\"  |\"CNT_CHILDREN_IND\"  |\"CNT_FAMILY_IND\"  |\"AGE\"  |\"WORKYEAR\"  |\"OCCUPATION_TYPE\"  |\"TARGET\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|5008804  |M              |Y               |Y                  |427500.00           |Working             |Higher education       |Civil marriage        |Rented apartment     |1             |1                  |0             |0             |0                   |2                 |33     |13          |OTHER              |0         |\n",
      "|5008805  |M              |Y               |Y                  |427500.00           |Working             |Higher education       |Civil marriage        |Rented apartment     |1             |1                  |0             |0             |0                   |2                 |33     |13          |OTHER              |0         |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_data = application_record.join(credit_record, using_columns=id_col, join_type='inner')\n",
    "combined_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "Store the combined table into feature view, such that it is automatically updated when the underlying tables (application record & credit record) being updated\n",
    "\n",
    "Go to Snowsight to view the dependency graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "|\"NAME\"         |\"VERSION\"  |\n",
      "-----------------------------\n",
      "|COMBINED_DATA  |V1         |\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fv_name = \"COMBINED_DATA\"\n",
    "fv_version = \"V1\"\n",
    "\n",
    "# Create the FeatureView instance\n",
    "fv_instance = FeatureView(\n",
    "    name=fv_name, \n",
    "    entities=[], \n",
    "    feature_df=combined_data,\n",
    "    refresh_freq='6 hours',           # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "    desc=\"Features & labels for training\"\n",
    ")\n",
    "\n",
    "# Register the FeatureView instance.  Creates  object in Snowflake\n",
    "fs.register_feature_view(\n",
    "    feature_view=fv_instance,\n",
    "    version=fv_version, \n",
    "    block=True\n",
    ")\n",
    "\n",
    "fs.list_feature_views().select('NAME', 'VERSION').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "View refresh history of the feature view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"            |\"STATE\"    |\"REFRESH_START_TIME\"              |\"REFRESH_END_TIME\"                |\"REFRESH_ACTION\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|COMBINED_DATA$V1  |SUCCEEDED  |2024-09-29 06:30:58.068000-07:00  |2024-09-29 06:30:59.968000-07:00  |INCREMENTAL       |\n",
      "|COMBINED_DATA$V1  |SUCCEEDED  |2024-09-29 05:43:47.478000-07:00  |2024-09-29 05:43:49.543000-07:00  |INCREMENTAL       |\n",
      "|COMBINED_DATA$V1  |SUCCEEDED  |2024-09-29 05:03:08.958000-07:00  |2024-09-29 05:03:11.271000-07:00  |INCREMENTAL       |\n",
      "|COMBINED_DATA$V1  |SUCCEEDED  |2024-09-29 03:46:39.530000-07:00  |2024-09-29 03:46:40.948000-07:00  |INCREMENTAL       |\n",
      "|COMBINED_DATA$V1  |SUCCEEDED  |2024-09-29 01:41:52.504000-07:00  |2024-09-29 01:41:54.147000-07:00  |INCREMENTAL       |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs.get_refresh_history(fv_name, fv_version).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "### Apply One-Hot Encoding, Ordinal Encoding, and Standardization\n",
    "\n",
    "Run the code below to apply one-hot encoding, ordinal encoding, and standardization on the feature vectors. \n",
    "\n",
    "If you encounter an error, try rerunning the code block.\n",
    "\n",
    "**Note:** The feature view is created before the transformation because the transformation pipeline generates temporary tables. The feature store doesn't support saving these transformations, which should ideally be saved in the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Warning: The Decimal(18, 2) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
      "UserWarning: Warning: The Decimal(18, 2) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n"
     ]
    }
   ],
   "source": [
    "combined_data = fs.get_feature_view(fv_name, fv_version).feature_df\n",
    "\n",
    "categorical_columns = ['CODE_GENDER', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', \n",
    "                        'CNT_CHILDREN_IND', 'CNT_FAMILY_IND', 'OCCUPATION_TYPE', 'NAME_HOUSING_TYPE']\n",
    "ordinal_columns = ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL']\n",
    "numeric_columns = ['AMT_INCOME_TOTAL', 'AGE', 'WORKYEAR']\n",
    "\n",
    "transformation_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"OneHot\", OneHotEncoder(input_cols=categorical_columns, output_cols=categorical_columns, drop_input_cols=True)),\n",
    "        (\"Ordinal\", OrdinalEncoder(input_cols=ordinal_columns, output_cols=ordinal_columns)),\n",
    "        (\"Scaler\", StandardScaler(input_cols=numeric_columns, output_cols=numeric_columns))\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_encoded = transformation_pipeline.fit(combined_data).transform(combined_data)\n",
    "session.use_schema(config.schema)\n",
    "data_encoded.write.save_as_table(table_name='CREDIT_RISK_PREPARED', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "### Handling Class Imbalance\n",
    "\n",
    "The dataset is imbalanced, with significantly more 'low-risk' than 'high-risk' customers. This can bias the model toward predicting the majority class ('low-risk'), leading to poor performance on the minority class ('high-risk').\n",
    "\n",
    "**Class imbalance** occurs when one class (e.g., 'low-risk') dominates the dataset, which can cause the model to favor that class and result in inaccurate predictions for the minority class.\n",
    "\n",
    "**Brainstorm:** Have you encountered similar situations where one category dominates and skews predictions? How did you handle it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "|\"TARGET\"  |\"COUNT\"  |\n",
      "----------------------\n",
      "|1         |503      |\n",
      "|0         |29819    |\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_encoded.group_by('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "# session.use_schema(config.schema)\n",
    "\n",
    "@sproc(name=\"sproc_oversample_smote\", is_permanent=True, stage_location=f\"@ML_PROCS\",\n",
    "        replace=True, packages=['snowflake-snowpark-python==1.20.0', 'scikit-learn==1.2.2', 'imbalanced-learn==0.12.3'])\n",
    "def sproc_oversample_smote(session: Session, training_table: str, feature_cols: T.List[str], target_col: str, target_table: str) -> T.Variant:\n",
    "    \"\"\"\n",
    "    A Snowpark stored procedure for oversampling data using SMOTE and writing it back to Snowflake.\n",
    "    \"\"\"\n",
    "    training_sdf = session.table(training_table)\n",
    "    training_pdf = training_sdf.to_pandas()\n",
    "\n",
    "    # Perform SMOTE oversampling\n",
    "    X_balance, y_balance = SMOTE().fit_resample(training_pdf[feature_cols], training_pdf[target_col])\n",
    "    X_balance[target_col] = y_balance\n",
    "\n",
    "    # Write the oversampled data back to Snowflake\n",
    "    session.sql(f\"DROP TABLE IF EXISTS {target_table}\").collect()\n",
    "    session.write_pandas(X_balance, table_name=target_table, auto_create_table=True)\n",
    "\n",
    "    return \"Successfully over-sampled\"\n",
    "\n",
    "\n",
    "# SMOTE Oversampling\n",
    "feature_cols = [col.strip('\"') for col in data_encoded.columns if col not in ['TARGET', 'ID']]\n",
    "\n",
    "# TODO: Call the stored procedure for SMOTE oversampling\n",
    "# Pass in the session, the input table, the feature columns, the target column, and the output table.\n",
    "sproc_oversample_smote(\n",
    "    session, \"CREDIT_RISK_PREPARED\", feature_cols, \"TARGET\", \"CREDIT_RISK_BALANCED\"\n",
    ")\n",
    "\n",
    "data_rebalanced = session.table(f\"CREDIT_RISK_BALANCED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "|\"TARGET\"  |\"COUNT\"  |\n",
      "----------------------\n",
      "|0         |29819    |\n",
      "|1         |29819    |\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_rebalanced.group_by('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "### Train test split and train model\n",
    "\n",
    "Cross-validation should be used for more reliable model evaluation., but this example uses a fixed 80-20 split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000026",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "### Build Two Model Versions\n",
    "\n",
    "1. Build the **first version** using prior-balancing `CREDIT_RISK_PREPARED`.\n",
    "2. Build the **second version** using after-balancing `CREDIT_RISK_BALANCED`.\n",
    "\n",
    "Compare the performance of the two versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce110000-1111-2222-3333-ffffff000027",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47574, 12064)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the CREDIT_RISK_PREPARED (v1) or CREDIT_RISK_BALANCED (v2) and split it into training and test sets\n",
    "# Use a random split with 80% for training and 20% for testing. Don't forget to set the random seed.\n",
    "\n",
    "# data_rebalanced = session.table(\"CREDIT_RISK_PREPARED\")\n",
    "data_rebalanced = session.table(\"CREDIT_RISK_BALANCED\")\n",
    "\n",
    "train_data, test_data = data_rebalanced.randomSplit([0.8, 0.2], seed=0)\n",
    "train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000028",
   "metadata": {
    "name": "cell29"
   },
   "source": [
    "### Train an XGBClassifier\n",
    "\n",
    "You can easily switch the classifier to `snowflake.ml.modeling.linear_model.LinearRegression`, `snowflake.ml.modeling.ensemble.RandomForestClassifier`, or others, and compare their performance.\n",
    "\n",
    "Simply update the classifier in the code and re-run the cell to observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce110000-1111-2222-3333-ffffff000029",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.ml.modeling.xgboost.xgb_classifier.XGBClassifier at 0x35abdcf90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [col for col in train_data.columns if col != target_col]\n",
    "\n",
    "# Model training\n",
    "# switch into classifier\n",
    "model = XGBClassifier(input_cols=feature_cols, label_cols=[target_col], output_cols=[output_col])\n",
    "_ = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000030",
   "metadata": {
    "name": "cell31"
   },
   "source": [
    "### Model Evaluation Metrics\n",
    "\n",
    "- **Accuracy**: The proportion of correctly predicted instances out of the total instances.\n",
    "- **Precision**: The proportion of true positive predictions out of all predicted positives (measures how accurate positive predictions are).\n",
    "- **Recall**: The proportion of true positive predictions out of all actual positives (measures the ability to detect positive instances).\n",
    "- **F1 Score**: The harmonic mean of precision and recall, balancing both to provide a single metric for performance.\n",
    "\n",
    "For a more detailed explanation, try prompting \"explain to me accuracy, confusion matrix, precision, recall, F1 score\" in ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce110000-1111-2222-3333-ffffff000031",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame.flatten() is deprecated since 0.7.0. Use `DataFrame.join_table_function()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.990633,\n",
       " 'Precision': 0.9900066622251832,\n",
       " 'Recall': 0.991162247790562,\n",
       " 'F1 Score': 0.9905841179901674}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(test_data)\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "    'Precision': precision_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "    'Recall': recall_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "    'F1 Score': f1_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT')\n",
    "}\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000032",
   "metadata": {
    "name": "cell33"
   },
   "source": [
    "### Save to model registry\n",
    "\n",
    "Go to Snowsight to see model object under HOL1_DB > HOL1_SCHEMA > Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce110000-1111-2222-3333-ffffff000033",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
      "UserWarning: Inferring model signature from sample input or providing model signature for Snowpark ML Modeling model is not required. Model signature will automatically be inferred during fitting. \n",
      "2024-09-29 23:41:17,976 - __main__ - INFO - Model CREDIT_RISK_XGBoost logged with version V1\n",
      "INFO:__main__:Model CREDIT_RISK_XGBoost logged with version V1\n"
     ]
    }
   ],
   "source": [
    "model_registry = Registry(session=session, database_name=config.database, schema_name=config.schema)\n",
    "\n",
    "# a helper function to get the latest model version, if not found, return V1\n",
    "def get_new_model_version(model_name: str) -> str:\n",
    "    try:\n",
    "        new_version = re.sub(r'\\bV(\\d+)\\b', lambda m: f\"V{int(m.group(1)) + 1}\", model_registry.get_model(model_name).last().version_name)\n",
    "    except:\n",
    "        new_version = \"V1\"\n",
    "    return new_version\n",
    "\n",
    "def log_model(model_name, model, metrics, version_name, sample_data):\n",
    "    model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=model,\n",
    "        metrics=metrics,\n",
    "        version_name=version_name,\n",
    "        sample_input_data=sample_data.limit(10)\n",
    "    )\n",
    "    print(f\"Model {model_name} logged with version {version_name}\")\n",
    "\n",
    "log_model(model_name, model, metrics, get_new_model_version(model_name), train_data.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000034",
   "metadata": {
    "collapsed": false,
    "name": "cell35"
   },
   "source": [
    "### Feature importance analysis\n",
    "\n",
    "XGBoost provides a **feature importance** score, which measures the contribution of each feature to the model's predictions. This is calculated based on how often and how effectively the feature is used in decision trees. Features with higher importance scores are more influential in driving the model's decisions.\n",
    "\n",
    "In the **Feature Importance** bar chart, we can see that `FLAG_OWN_REALTY` has the highest importance score. This means that owning real estate has a strong influence on the model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000035",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "model_local = model.to_xgboost()\n",
    "\n",
    "# Plot feature importance\n",
    "feat_importance = pd.DataFrame(model_local.feature_importances_,model_local.feature_names_in_,columns=['FeatImportance'])\n",
    "feat_importance.sort_values('FeatImportance').plot.barh(y='FeatImportance', figsize=(5,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000036",
   "metadata": {
    "collapsed": false,
    "name": "cell37"
   },
   "source": [
    "### SHAP Values\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** values provide a way to understand the impact of each feature on the model’s output for individual predictions. SHAP assigns each feature a value representing its contribution to pushing the prediction away from the base value (mean prediction). \n",
    "\n",
    "In the **SHAP summary plot** below, the color represents the feature's value (blue for low and pink for high). The SHAP values indicate how much each feature contributes to increasing or decreasing the prediction. For example, higher values of `AGE` (shown in pink) tend to increase the predicted risk, while lower values (in blue) decrease it.\n",
    "\n",
    "In both graphs, we see that `FLAG_OWN_REALTY` and `AGE` are important features, but the SHAP plot shows us more context. For instance, in the SHAP plot, older age (higher values of `AGE`, pink) is shown to increase the risk, while younger age (blue) lowers the risk. This kind of insight helps explain why the feature is important and how it influences specific predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000037",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "X_data = train_data.to_pandas()\n",
    "explainer = shap.Explainer(model_local, X_data)\n",
    "shap_values = explainer(X_data)\n",
    "shap.summary_plot(shap_values, X_data, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000038",
   "metadata": {
    "name": "cell39"
   },
   "source": [
    "### Build a Streamlit App for Single Customer Inference\n",
    "\n",
    "In this step, we will set up a Streamlit app to perform inference on a new customer. \n",
    "\n",
    "1. Run the two cells below to create a stored procedure that will handle the inference.\n",
    "2. Once the stored procedure is set up, we will integrate it with the Streamlit app to process customer data and return predictions.\n",
    "\n",
    "Go ahead and run the following two cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000039",
   "metadata": {
    "name": "cell40"
   },
   "source": [
    "\n",
    "This stored procedure, `sproc_predict_new_customer`, performs inference on new customer data using an XGBoost model:\n",
    "\n",
    "1. **Model Versioning**: Fetches the latest model version from the registry if none is provided.\n",
    "2. **Data Handling**: Converts the new customer records into a Snowpark DataFrame.\n",
    "3. **Duplicate Check**: Leftanti joins with the `prediction_table` to ensure only new records are processed.\n",
    "4. **Feature Engineering & Prediction**: Applies transformations, runs predictions, and appends results to the `prediction_table`.\n",
    "5. **Return Predictions**: Returns all predictions (both new and existing) made for the given customer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce110000-1111-2222-3333-ffffff000040",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "@sproc(name=\"sproc_predict_new_customer\", is_permanent=True, stage_location=\"@ML_PROCS\",\n",
    "       replace=True, packages=['snowflake-snowpark-python', 'snowflake-ml-python==1.6.1', 'xgboost==1.7.6', 'pandas'],\n",
    "       imports=[\"@ML_PROCS/utils.py.gz\"]\n",
    "    )\n",
    "def predict_new_customer(\n",
    "    session: Session,\n",
    "    new_customer_record: list[dict],  # Single dict for one customer, list of dicts for batch inference\n",
    "    database_name: str = 'HOL1_DB',\n",
    "    schema_name: str = 'HOL1_SCHEMA',\n",
    "    model_name: str = 'CREDIT_RISK_XGBOOST',\n",
    "    prediction_table: str = 'PREDICTION_RESULTS',\n",
    "    model_version: str = None\n",
    ") -> str:\n",
    "    from utils import feature_engineering\n",
    "    from snowflake.snowpark.functions import col, lit\n",
    "\n",
    "    # Initialize model registry and fetch the latest version if not provided\n",
    "    model_registry = Registry(session=session, database_name=database_name, schema_name=schema_name)\n",
    "    model_version = model_version or model_registry.get_model(model_name).last().version_name\n",
    "\n",
    "    # If it's a single record, convert it into a Snowpark DataFrame\n",
    "    new_customer_record = session.create_dataframe(new_customer_record)\n",
    "\n",
    "    # Add model_version as a column to the customer records for the join\n",
    "    customer_with_version = new_customer_record.with_column(\"MODEL_VERSION\", lit(model_version))\n",
    "\n",
    "    # Left join to find records in single_customer_record that are not in prediction_table\n",
    "    prediction_table_df = session.table(prediction_table).alias(\"pred_tbl\")\n",
    "    records_to_predict = customer_with_version.join(\n",
    "        prediction_table_df,\n",
    "        on=[\"ID\", \"MODEL_VERSION\"],\n",
    "        how=\"leftanti\"  # Keep only records that do not exist in prediction_table\n",
    "    )\n",
    "\n",
    "    # If there are any records that need prediction\n",
    "    if records_to_predict.count() > 0:\n",
    "        model = model_registry.get_model(model_name).version(model_version).load()\n",
    "\n",
    "        # Apply feature engineering and transformation pipeline\n",
    "        records_to_predict = transformation_pipeline.transform(feature_engineering(records_to_predict))\n",
    "        \n",
    "        # Make predictions\n",
    "        prediction_results = model.predict(records_to_predict).with_column(\"MODEL_VERSION\", lit(model_version))\n",
    "        prediction_results.select(\"ID\", \"PREDICT\", \"MODEL_VERSION\").write.save_as_table(prediction_table, mode=\"append\")\n",
    "\n",
    "    # Return all predictions (both existing and new)\n",
    "    all_predictions = session.table(prediction_table).filter(\n",
    "        col(\"ID\").cast(\"STRING\").in_(new_customer_record.select(col(\"ID\").cast(\"STRING\"))) &\n",
    "        (col(\"MODEL_VERSION\") == lit(model_version))\n",
    "    ).select(\"ID\", \"PREDICTION\")\n",
    "    all_predictions_dict = str(all_predictions.to_pandas().set_index('ID')['PREDICTION'].to_dict())\n",
    "\n",
    "    return all_predictions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000041",
   "metadata": {
    "name": "cell42"
   },
   "source": [
    "Follow these steps to deploy and run the Streamlit app in Snowsight:\n",
    "\n",
    "1. Navigate to Snowsight > Projects > Streamlit, and create a new app. Make sure you choose to deploy using role HOL1, with the app location set to HOL1_DB.HOL1_SCHEMA.\n",
    "2. Copy and paste the contents of `app.py` into the new app.\n",
    "3. Before running the app, ensure that the snowflake-ml-python package is selected in the list of dependencies.\n",
    "4. Run the app, then click **\"Predict\"** to generate predictions for the default customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000042",
   "metadata": {
    "name": "cell43"
   },
   "source": [
    "Based on the **feature importance analysis**, identify which features have the greatest impact on the model's predictions. You can use the **Streamlit** app to explore and interact with different prediction results.\n",
    "\n",
    "**Note**: In the current implementation, if an existing `(ID, model_version)` pair is found in the `prediction_table`, the feature will **not** be re-evaluated, even if the underlying data has changed. To view updated predictions, ensure you select a **new ID** along with the desired model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000043",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "session.table('PREDICTION_RESULTS').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000044",
   "metadata": {
    "name": "cell45"
   },
   "source": [
    "### Batch inference pipeline\n",
    "\n",
    "This pipeline performs batch predictions, evaluates the model, and triggers retraining if the F1 score falls below a threshold.\n",
    "\n",
    "1. **`batch_predict_and_evaluate`**: Transforms features, predicts, evaluates F1 score, and triggers retraining if F1 score < 70%.\n",
    "2. **`append_data_to_original_tables`**: Appends new records and refreshes the feature view.\n",
    "3. **`retrain_model`**: Retrieves feature view, applies transformations, retrains the model, and logs a new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce110000-1111-2222-3333-ffffff000045",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": [
    "def feature_engineering(application_record):\n",
    "    \"\"\"\n",
    "    Applies feature engineering to the application record.\n",
    "    \"\"\"\n",
    "    return application_record.with_column(\n",
    "        'CNT_CHILDREN_IND', F.iff(F.col('CNT_CHILDREN') >= 2, \"2+\", F.to_varchar(F.col('CNT_CHILDREN')))\n",
    "    ).drop('CNT_CHILDREN').with_column(\n",
    "        'CNT_FAMILY_IND', F.iff(F.col('CNT_FAM_MEMBERS') >= 3, \"3+\", F.to_varchar(F.col('CNT_FAM_MEMBERS')))\n",
    "    ).drop('CNT_FAM_MEMBERS').with_column(\n",
    "        'AGE', F.abs(F.floor(F.col('DAYS_BIRTH') / 365))\n",
    "    ).drop('DAYS_BIRTH').with_column(\n",
    "        'WORKYEAR', F.abs(F.floor(F.col('DAYS_EMPLOYED') / 365))\n",
    "    ).filter(F.col('WORKYEAR') < 50).drop('DAYS_EMPLOYED').with_column(\n",
    "        'OCCUPATION_TYPE',\n",
    "        F.iff(F.col('OCCUPATION_TYPE').in_(['Cleaning staff', 'Cooking staff', 'Drivers', 'Laborers', 'Low-skill Laborers', 'Security staff', 'Waiters/barmen staff']), 'LABOURWORK',\n",
    "              F.iff(F.col('OCCUPATION_TYPE').in_(['Accountants', 'Core staff', 'HR staff', 'Medicine staff', 'Private service staff', 'Realty agents', 'Sales staff', 'Secretaries']), 'OFFICEWORK', \n",
    "                    F.iff(F.col('OCCUPATION_TYPE').in_(['Managers', 'High skill tech staff', 'IT staff']), 'HIGHTTECHWORK', 'OTHER')))\n",
    "    )\n",
    "\n",
    "def batch_predict_and_evaluate(\n",
    "    session: Session, new_application_record_name: str, new_credit_record_name: str, model_name: str, model_version: str = None,\n",
    "    database_name: str = 'HOL1_DB',\n",
    "    schema_name: str = 'HOL1_SCHEMA',\n",
    "    threshold: float = 0.7\n",
    "):\n",
    "    # Combine data loading and transformation\n",
    "    new_application_record = transformation_pipeline.transform(\n",
    "        feature_engineering(session.table(new_application_record_name))\n",
    "    )\n",
    "\n",
    "    # Combine model loading and version selection\n",
    "    model_version = model_version or model_registry.get_model(model_name).last().version_name\n",
    "    model = model_registry.get_model(model_name).version(model_version).load()\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_df = model.predict(new_application_record).select(F.col('ID'), F.col('PREDICT'))\n",
    "\n",
    "    # Process labels\n",
    "    new_credit_record = process_labels(session.table(new_credit_record_name))\n",
    "\n",
    "    # Join predictions with labels and calculate F1 score\n",
    "    evaluation_df = predictions_df.join(new_credit_record, on=\"ID\")\n",
    "    f1 = f1_score(df=evaluation_df, y_true_col_names='TARGET', y_pred_col_names='PREDICT')\n",
    "\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    \n",
    "    # Check if retraining is needed\n",
    "    if f1 < threshold:\n",
    "        append_data_to_original_tables(session, new_application_record_name, new_credit_record_name)\n",
    "        print(f\"Starting re-training... from model version: {model_version}\")\n",
    "        retrain_model(session, model_name, model_version)\n",
    "    else:\n",
    "        # Calculate and log metrics\n",
    "        metrics = {\n",
    "            'Accuracy': accuracy_score(df=evaluation_df, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "            'Precision': precision_score(df=evaluation_df, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "            'Recall': recall_score(df=evaluation_df, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "            'F1 Score': f1\n",
    "        }\n",
    "        print(f\"Metrics (Accuracy: {metrics['Accuracy']}, Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, F1: {metrics['F1 Score']})\")\n",
    "\n",
    "def append_data_to_original_tables(session: Session, new_application_record_name: str, new_credit_record_name: str):\n",
    "    # Load new data\n",
    "    new_application_record = session.table(new_application_record_name)\n",
    "    new_credit_record = session.table(new_credit_record_name)\n",
    "\n",
    "    # Fetch table count once and append data\n",
    "    table = session.table(f\"{config.database}.{config.schema}.{application_record_table_name}\")\n",
    "    count_before = table.count()\n",
    "\n",
    "    new_application_record.write.save_as_table(application_record_table_name, mode=\"append\")\n",
    "    new_credit_record.write.save_as_table(credit_record_table_name, mode=\"append\")\n",
    "\n",
    "    print(f\"Data length before append: {count_before}, after append: {table.count()}\")\n",
    "\n",
    "    # Refresh feature view\n",
    "    fs.refresh_feature_view(fv_name, fv_version)\n",
    "    print(f'Feature view {fv_name} refreshed')\n",
    "\n",
    "def retrain_model(session: Session, model_name, model_version):\n",
    "    # Get updated feature view\n",
    "    combined_data = fs.get_feature_view(fv_name, fv_version).feature_df\n",
    "    \n",
    "    # Encode and save data\n",
    "    data_encoded = transformation_pipeline.fit(combined_data).transform(combined_data)\n",
    "    data_encoded.write.save_as_table(table_name='CREDIT_RISK_PREPARED', mode='overwrite')\n",
    "    \n",
    "    # SMOTE Oversampling\n",
    "    feature_cols = [col.strip('\"') for col in data_encoded.columns if col not in ['TARGET', 'ID']]\n",
    "    sproc_oversample_smote(session, \"CREDIT_RISK_PREPARED\", feature_cols, \"TARGET\", \"CREDIT_RISK_BALANCED\")\n",
    "    data_rebalanced = session.table(\"CREDIT_RISK_BALANCED\")\n",
    "\n",
    "    # Split data for training and testing\n",
    "    train_data, test_data = data_rebalanced.randomSplit([0.8, 0.2], seed=0)\n",
    "\n",
    "    # Load the model and retrain\n",
    "    model = model_registry.get_model(model_name).version(model_version).load()\n",
    "    model.fit(train_data)\n",
    "    print(f\"Retrained model with version: {model_version}\")\n",
    "\n",
    "    result = model.predict(test_data)\n",
    "\n",
    "    # Model evaluation and logging\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "        'Precision': precision_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "        'Recall': recall_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT'),\n",
    "        'F1 Score': f1_score(df=result, y_true_col_names='TARGET', y_pred_col_names='PREDICT')\n",
    "    }\n",
    "\n",
    "    # Increment model version and log\n",
    "    new_version = re.sub(r'\\bV(\\d+)\\b', lambda m: f\"V{int(m.group(1)) + 1}\", model_registry.get_model(model_name).last().version_name)\n",
    "    log_model(model_name, model, metrics, new_version, train_data)\n",
    "    print(f'Model retrained successfully with updated metrics {metrics}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000046",
   "metadata": {
    "name": "cell47"
   },
   "source": [
    "### Trigger New Model with Synthetic Data\n",
    "\n",
    "Use the **synthetically generated** batch of data to trigger the model evaluation. \n",
    "\n",
    "Since the data is synthetic, it is expected to produce a **very low F1 score**, which will trigger model retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000047",
   "metadata": {
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": [
    "batch_predict_and_evaluate(\n",
    "    session, 'NEW_APPLICATION_RECORD', 'NEW_CREDIT_RECORD', 'CREDIT_RISK_XGBoost'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000048",
   "metadata": {
    "name": "cell49"
   },
   "source": [
    "Verify the the feature view has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000049",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": [
    "fs.get_refresh_history(fv_name, fv_version).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000050",
   "metadata": {
    "name": "cell51"
   },
   "source": [
    "Verify new model version is logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000051",
   "metadata": {
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000052",
   "metadata": {
    "name": "cell53"
   },
   "source": [
    "### View Updated Results in Streamlit\n",
    "\n",
    "Navigate to the **Streamlit** app and select **model version V2** to view the updated prediction results based on the latest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce110000-1111-2222-3333-ffffff000053",
   "metadata": {
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000054",
   "metadata": {
    "language": "python",
    "name": "cell55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311snowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
